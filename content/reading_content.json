{
  "rm_c1_prompting": {
    "content_id": "rc_rm_c1_prompting",
    "course_id": "rm_c1_prompting",
    "concept_text": "Great AI output starts with a great prompt. The CRAF framework gives you four elements that consistently produce usable output:\n\n**C — Context**: Who is the client? What situation are they in? What do you already know?\nExample: 'A $45M Ontario manufacturer exporting to the US and Germany, no current EDC relationship.'\n\n**R — Role**: What role should the AI play?\nExample: 'You are a senior Relationship Manager at a Canadian export finance institution.'\n\n**A — Action**: What exactly do you want it to do?\nExample: 'Draft a 200-word discovery brief with the key questions I should ask in my first call.'\n\n**F — Format**: How should the output be structured?\nExample: 'Use three sections: Business Context, Key Discovery Questions, and Recommended Next Step.'\n\nWhen all four elements are present, the AI knows who it is speaking as, who it is speaking about, what to produce, and how to present it. Missing any one element degrades the output — often dramatically.",
    "good_example": "**Prompt:** 'Context: Maple Industries Ltd., a $45M Ontario manufacturer, was just handed off from our ARM team. They export to the US and Germany. They have no current EDC relationship and currently use another bank's letter of credit facility. Role: You are a senior Relationship Manager at a Canadian export finance institution. Action: Draft a 200-word discovery brief with the key questions I should ask in my first call. Format: Use three sections — Business Context, Key Discovery Questions, and Recommended Next Step.'\n\n**Why it works:** The AI has a specific company profile (Context), a clear voice (Role), a concrete deliverable (Action), and knows exactly how to structure the output (Format). The result is usable with minor edits — not a 500-word generic overview of export finance.",
    "anti_pattern": "**Prompt:** 'Write a discovery brief for my new client.'\n\n**Why it fails:** The AI has no context about the client, no role to speak from, no specific deliverable, and no format guidance. The output will be a generic template applicable to any client — which means it's useful to no one. The RM will spend more time editing than they saved by using AI. This is the single most common prompting mistake: treating AI like a search engine instead of a collaborator who needs context.",
    "takeaway": "A prompt is only as useful as the context you put in it. Specificity in all four CRAF elements — Context, Role, Action, Format — is what separates AI output you can use from output you have to rewrite."
  },
  "rm_c2_verification": {
    "content_id": "rc_rm_c2_verification",
    "course_id": "rm_c2_verification",
    "concept_text": "AI meeting recap tools (like Copilot in Teams) generate summaries from audio and transcripts. They are fast — but error-prone in specific and predictable ways:\n\n**Common hallucination types in meeting recaps:**\n- **Wrong title**: 'CEO' when the person is a CFO\n- **Invented commitment**: 'committed to sending by Friday' when they only 'mentioned it'\n- **Fabricated figure**: '$3.5M facility' when the client said 'about three million'\n- **Unconfirmed plan**: 'expanding to Germany' when Germany was mentioned as a possibility\n- **Invented meeting**: 'follow-up scheduled for Nov 21' when no date was set\n\n**The verification checklist — check these four things before logging anything to C3:**\n1. **Dates**: Are all dates mentioned in the recap verifiable from your own notes or a confirmed calendar invite?\n2. **Names and titles**: Are all names spelled correctly and titles accurate?\n3. **Commitments**: Did the person actually commit, or did they 'mention', 'consider', or 'suggest'?\n4. **Figures**: Are all dollar amounts, percentages, and numerical references verifiable from your notes?",
    "good_example": "Copilot generated: 'Client mentioned interest in a $2M facility renewal.' Before logging, I checked my notes — the client actually said 'possibly around two million, but we haven't finalized.' I changed the CRM note to: 'Client indicated a potential facility renewal in the $1.5–2M range, subject to board confirmation. To be confirmed at follow-up.' Result: no false commitment in C3, and the follow-up note is actionable.",
    "anti_pattern": "Copilot generated: 'Sarah Chen committed to sending the financial statements by Friday.' I copied this directly into C3 without checking my notes. In my notes I had only written 'Sarah mentioned statements' with no deadline. Sarah never received a follow-up request because she never made that commitment — but C3 showed it as a pending action item for two weeks before I noticed. This created confusion with my ARM and eroded trust in the CRM data.",
    "takeaway": "Every AI-generated recap must pass four checks before it touches C3 or any permanent record: dates, names and titles, commitments, and figures. What you can't verify from your own notes should not be logged."
  },
  "rm_c3_data_safety": {
    "content_id": "rc_rm_c3_data_safety",
    "course_id": "rm_c3_data_safety",
    "concept_text": "EDC's GenAI governance policy draws a clear line: do not input non-public information into AI tools that are not approved to handle it. For RMs, the risk is real and specific — C3 contains confidential client data that clients share with EDC in confidence.\n\n**What counts as non-public information:**\n- Client credit facility amounts and terms\n- Deal structures, pricing, and transaction specifics\n- Client expansion plans mentioned in confidence\n- NPS scores and relationship health data\n- Verbatim CRM notes and relationship observations\n- Any client identifier combined with financial data\n\n**The public/non-public test:**\nBefore including any data in an AI prompt, ask: 'Could this information appear in a press release without causing a compliance issue?' If the answer is no — or even 'maybe' — abstract it first.\n\n**The abstraction technique:**\n- Replace client names with roles: 'a mid-market manufacturing client in Ontario'\n- Round or range-ize figures: '$2.8M' becomes 'approximately $2–3M' or 'a mid-size facility'\n- Replace deal-specific dates with quarters: 'expired September 2024' becomes 'expired in Q3 2024'\n- Remove relationship observations that identify the client relationship: 'exploring alternatives due to pricing' becomes 'a client whose facility expired and has not renewed'",
    "good_example": "Original prompt (unsafe): 'Maple Industries Ltd. has a $4.5M BCAP facility expiring March 2025. Write a renewal outreach email.'\n\nSafe version: 'One of my mid-market manufacturing clients in Ontario has an export finance facility expiring in Q1 2025. Write a renewal outreach email that emphasizes continuity of service and long-term relationship value. Format: 150 words, professional tone, no product-specific jargon.'\n\nWhy it works: The AI can write a perfectly useful, personalized-feeling email without knowing the client name, the exact amount, or the product type. The RM then personalizes manually before sending.",
    "anti_pattern": "An RM copied the full C3 client profile — company name, credit facility amount, deal history, NPS score, and relationship notes — and pasted it into Copilot on the Web with the prompt: 'Write me an account strategy for this client.' The output was useful, but the method put confidential client data into an unapproved AI surface. This is a direct violation of EDC's GenAI policy, regardless of whether the output was ever used.",
    "takeaway": "Before hitting send on any AI prompt, ask: 'Could this information appear in a press release?' If not, abstract it first. The prompt can still be useful — often more so — without the specific details."
  },
  "rm_c4_tool_fluency": {
    "content_id": "rc_rm_c4_tool_fluency",
    "course_id": "rm_c4_tool_fluency",
    "concept_text": "M365 Copilot is embedded across the tools you already use every day. The key skill is knowing which surface to reach for — and in what order.\n\n**Teams Copilot** — best for:\n- Summarizing meetings you attended or missed\n- Extracting action items and key decisions from transcripts\n- Getting a quick briefing on a long call\n\n**Outlook Copilot** — best for:\n- Drafting and replying to emails\n- Summarizing long email threads\n- Generating follow-up messages with clear next steps\n\n**Excel Copilot** — best for:\n- Analyzing exported data (pipeline, portfolio, prospect lists)\n- Identifying patterns, outliers, and priorities in structured data\n- Creating summaries and pivot-style insights from a dataset\n\n**Word / SharePoint Copilot** — best for:\n- Structuring unstructured content into a document format\n- Drafting briefing notes, one-pagers, and summaries from scratch\n- Turning a set of bullets into a polished narrative\n\n**Multi-step workflow principle:**\nOutput from one surface becomes the input for the next. A standard RM Monday workflow: Teams recap (extract information) → Word briefing note (structure it) → Outlook draft (communicate it). Each step takes approximately 2–3 minutes instead of 8–10.",
    "good_example": "Monday morning, 8:45am. Jordan had a missed Friday call from Eastport Composites Ltd.:\n(1) Teams Copilot: 'Summarize the missed call from Eastport Composites on Friday. List: key topics discussed, any commitments made by either party, and suggested next steps.' — 2 min\n(2) Word Copilot: Pasted the summary. 'Structure this as a one-page briefing note with sections: Client Context, Discussion Summary, Commitments, and My Recommended Next Step.' — 3 min\n(3) Outlook Copilot: 'Using the attached briefing note as context, draft a follow-up email to the CFO at Eastport Composites. Tone: professional and relationship-focused. Include: acknowledgement of the call, a clear next step with a proposed timeline, and an offer to connect this week.' — 2 min\nTotal: 7 minutes. Jordan was fully caught up and had an outbound email ready before 9am.",
    "anti_pattern": "Jordan tried to use Outlook Copilot to summarize a Teams meeting he missed. Outlook doesn't have access to Teams transcripts — it returned a generic message. He then tried to forward the Teams meeting invite to himself and ask Copilot to summarize the 'attached meeting' — no transcript, so it failed again. He ended up typing the recap manually from memory. The correct starting point was Teams Copilot, which has direct transcript access. Using the wrong surface for the input type means starting over.",
    "takeaway": "Match the tool to the input type: audio and transcripts belong in Teams; email threads belong in Outlook; spreadsheet data belongs in Excel; document drafting belongs in Word. Chain them in sequence — each step's output is the next step's input."
  },
  "rm_c5_capstone": {
    "content_id": "rc_rm_c5_capstone",
    "course_id": "rm_c5_capstone",
    "concept_text": "A win-back program starts with data from C3 — lapsed clients, facility types, expiry dates, last contact records. The challenge: that data is non-public and cannot go directly into any AI tool that isn't approved for confidential data.\n\n**The safe win-back workflow — in this exact order:**\n\n1. **Abstract first**: Replace client names with roles (e.g., 'mid-market food processor'), round figures to ranges, replace specific dates with quarters, remove verbatim relationship notes.\n\n2. **Analyze with Excel Copilot**: Upload the abstracted dataset. Ask Excel Copilot to identify segments with the highest concentration of expiring facilities, patterns in lapse reasons, or priority tiers by approximate facility size.\n\n3. **Structure insights with Word Copilot**: Turn the Excel analysis into a targeting brief or prioritization summary.\n\n4. **Draft outreach with Outlook Copilot**: Write a CRAF prompt for each priority segment. Use abstracted client descriptions, not real names or figures.\n\n5. **Verify before sending**: Check all output against your own records. Delete any figure or claim you did not provide — AI will sometimes invent plausible-sounding details about clients it knows nothing about.\n\nAt every step: no raw C3 data, no real client names, no unverified figures in anything that leaves your drafts folder.",
    "good_example": "Jordan ran a win-back program for 15 lapsed manufacturing clients:\n(1) Abstracted the C3 export: removed names, rounded amounts to size categories (small/mid/large), replaced specific dates with quarters, generalized notes to 'pricing' or 'competitive loss' or 'internal pause'. — 10 min\n(2) Excel Copilot on abstracted data: 'Analyze this dataset of lapsed clients. Identify which facility type and size category has the highest concentration of Q3-Q4 2024 expirations. Rank by win-back likelihood based on lapse reason.' — 3 min\n(3) Outlook CRAF prompt: 'Context: mid-market food processing clients whose BCAP facility expired in the last 6 months due to pricing. Role: you are a senior RM at a Canadian export finance institution. Action: draft a re-engagement email that acknowledges it's been a while, references the value of BCAP without naming pricing. Format: 120 words, warm but professional.'\n(4) Verified output: deleted one claim about 'current market rates' that Jordan never provided — likely hallucinated. Personalized each email manually before sending. — 5 min",
    "anti_pattern": "An RM exported the full C3 portfolio list — real client names, exact facility amounts, NPS scores, relationship notes — directly into Excel and uploaded it to Copilot Chat for analysis. The output was accurate and useful. But the method put confidential client data into an unapproved AI surface. Regardless of the output quality, this is a policy violation. The analysis should be redone on an abstracted dataset.",
    "takeaway": "The sequence matters as much as the tools: abstract first, then analyze, then draft, then verify. Skipping abstraction or verifying last are the two most common compliance and quality failures in AI-assisted portfolio work."
  },
  "uw_c1_prompting": {
    "content_id": "rc_uw_c1_prompting",
    "course_id": "uw_c1_prompting",
    "concept_text": "Underwriters don't need polished prose — they need usable first drafts that survive compliance review and reduce rework cycles. The **CRAF Framework** gives you four elements that consistently produce credit-decision emails you can actually send.\n\n**C — Context**: What is the situation? Name the product, the decision type, and what facts are in scope. Critically, state what the AI must *not* use — internal risk ratings, non-public thresholds, or confidential model outputs.\nExample: *'Credit insurance limit decision. Outcome is partial approval with conditions due to limited recent financial information. Do NOT reference internal scores or non-public sources.'*\n\n**R — Role**: Tell the AI who it is writing as.\nExample: *'You are an underwriter drafting a client-facing email in plain, professional language — not a sales pitch.'*\n\n**A — Action**: Define the exact deliverable — length, decision type, and required content.\nExample: *'Draft a 180–220 word email explaining the partial approval, listing the conditions, and describing what information could support reconsideration.'*\n\n**F — Format**: Force a reviewable structure so you can scan quickly and catch problems before sending.\nExample: *'Use three headings: Decision / Why / What you can do next. Use bullet points under the final heading.'*\n\nIn Copilot (Outlook or Word), CRAF is your guardrail against two common failures: the AI inventing rationale because you left the context vague, and the AI leaking internal language because you never told it what to exclude. Pair CRAF with your team's approved wording patterns and you start every draft close to the finish line.",
    "good_example": "**Scenario**: Underwriter needs to tell Apex Trading Co. their credit insurance limit was partially approved with conditions.\n\n**Without CRAF (typed into Copilot):** *'Write an email about the credit limit decision.'*\nResult: Copilot produces a vague, three-paragraph letter that invents a reason ('based on our proprietary risk assessment model'), uses internal jargon, and omits next steps entirely. The underwriter spends 20 minutes rewriting from scratch.\n\n**With CRAF (typed into Copilot in Outlook):**\n*'Context: Credit insurance limit decision for an exporter. Outcome is partial approval with conditions due to limited recent financial information. Do NOT mention internal risk ratings, thresholds, or non-public sources. Role: You are an underwriter writing a plain-language client email — factual and courteous, not promotional. Action: Draft a 180–220 word email explaining the decision, listing two to three conditions, and describing what additional information would support reconsideration. Format: Three headings — Decision / Why / What you can do next — with bullet points under the final heading.'*\nResult: Copilot returns a structured, reviewable draft that matches approved wording patterns. The underwriter makes two minor edits and sends within five minutes.",
    "anti_pattern": "**The vague action prompt**: Typing *'Write an email telling the client why we declined'* into Copilot with no further instruction.\n\nWhat goes wrong: Copilot has no context, no constraints, and no format guidance. It will often invent a plausible-sounding reason (sometimes referencing a 'risk model' or 'internal assessment'), use language that implies a final, unappealable decision, and produce a generic block of text with no structure. The underwriter now faces three risks: (1) a compliance flag because the draft implies internal methodology was disclosed, (2) a client complaint because the tone reads as dismissive, and (3) wasted time because the output requires a full rewrite. Skipping CRAF doesn't save time — it moves the work downstream and adds risk.",
    "takeaway": "CRAF turns Copilot from an unpredictable drafter into a controlled assistant that produces structured, compliance-safe credit decision emails you can review and send with confidence."
  },
  "uw_c3_data_safety": {
    "content_id": "rc_uw_c3_data_safety",
    "course_id": "uw_c3_data_safety",
    "concept_text": "Underwriters work with some of the most sensitive data in the organization — financial forecasts, draft term sheets, internal risk ratings, and personal guarantor information. Time pressure is exactly when data incidents happen: a deadline looms, a document lands in your inbox, and the fastest path feels like pasting everything into an AI tool and asking for a summary.\n\nThe **SAFE Abstraction Method** is a four-step discipline that lets you get real AI assistance without exposing non-public client information.\n\n**S — Scan for sensitive content:** Before writing any prompt, identify what must stay out. This includes client names, deal identifiers, exact financial figures, term sheet clauses, internal credit ratings, and any personal data (e.g., guarantor details).\n\n**A — Abstract and anonymize:** Replace specifics with roles and ranges. 'IronPeak Renewables Ltd.' becomes 'a renewable energy exporter.' A '$12.4M guarantee' becomes 'a mid eight-figure guarantee.' Remove any fact that, alone or combined, could identify the client or deal.\n\n**F — Frame the question safely:** Ask AI for structure, checklists, or analytical questions — not for risk decisions, approval recommendations, or ratings. AI helps you think; you make the call.\n\n**E — Execute in an approved environment and Evaluate:** Use M365 Copilot on stored, appropriately classified documents when policy permits. For everything else, use abstracted prompts only. Always review AI output before acting on it — policy requires human judgment at every decision point.\n\nSAFE works because it changes *what you share* and *what you ask for*, not whether you use AI at all. You stay efficient and compliant at the same time.",
    "good_example": "**Before (unsafe):** An underwriter pastes the following into a public-facing AI chatbot: 'Here is the full term sheet and three-year financial forecast for NorthStar Solar Corp. Summarize the key risks and tell me whether we should approve the $9.8M guarantee.'\n\n**After (SAFE — using abstracted prompt in M365 Copilot):** 'A renewable energy exporter in an emerging market is seeking a guarantee in the high seven-figure range. The draft term sheet includes restrictive covenants tied to construction milestones and tight drawdown deadlines. Provide a checklist of due-diligence questions I should ask to validate cash-flow assumptions, covenant feasibility, and construction timeline risk. Do not recommend approval or decline.'",
    "anti_pattern": "An underwriter receives a large financial package late on a Friday and, under deadline pressure, pastes the client's full three-year revenue forecast, the draft term sheet with deal-specific covenants, and the client's legal name directly into an external AI tool to generate a quick risk summary. The output is useful, but the non-public client data has now been processed outside approved systems. This is a policy breach, a potential disclosure of protected information, and — depending on the client's jurisdiction — may trigger regulatory notification obligations. The time saved is vastly outweighed by the institutional and reputational risk created.",
    "takeaway": "SAFE keeps AI useful under pressure by changing what you share and what you ask for — so you get analytical help without turning a tight deadline into a data incident."
  },
  "uw_c2_verification": {
    "content_id": "rc_uw_c2_verification",
    "course_id": "uw_c2_verification",
    "concept_text": "AI can summarize a company profile in seconds and sound authoritative doing it — but confidence in tone is not the same as accuracy. In underwriting, a wrong number or an invented claim in a credit file is not a minor error; it is an audit finding, a decision risk, and potentially a compliance issue. Your default stance must be: **use AI to accelerate reading, not to replace judgment**.\n\nThe **VERIFY Checklist** is a six-step routine you apply before any AI-generated output enters a credit submission, a risk recommendation, or client communication:\n\n**V — Validate sources:** Can you point to an actual document — financial statement, third-party report, term sheet — for every claim the AI makes? If not, the claim does not belong in the file.\n\n**E — Evaluate numbers:** Recalculate or cross-check key figures and ratios (revenue growth, leverage, DSCR) against source documents. AI can transpose, round, or fabricate figures.\n\n**R — Review policy and authority:** Does the AI's recommendation align with underwriting policy and your delegated authority (DOA)? AI cannot know your signing limits — you must apply them.\n\n**I — Identify missing information:** What evidence is still needed to support the conclusion? Flag gaps explicitly rather than letting the AI's confident tone paper over them.\n\n**F — Flag uncertainty:** Anything you cannot verify must be marked 'Unverified' and removed from decision text before it is shared or filed.\n\n**Y — Yield to escalation when unsure:** If uncertainty touches compliance, sanctions/KYC, or material exposure, escalate. Guessing is not an option in credit insurance.\n\nApply VERIFY every time — not just when the output looks suspicious. Polished formatting is not a substitute for a traceable source.",
    "good_example": "**Before VERIFY:** Copilot produces a one-page company assessment for Northfield Grain Holdings. It states: 'Revenue grew 18% YoY, placing the company in a strong liquidity position,' and concludes 'approve with standard covenants.'\n\n**After applying VERIFY:**\n- **V:** The 18% revenue growth claim has no footnote. You search the uploaded financials and find no matching figure — you mark it 'Unverified — no source document.'\n- **E:** You recalculate the current ratio from the balance sheet yourself; the AI's stated ratio of 2.1 is actually 1.6 once you use the correct current liabilities line.\n- **R:** The 'approve with standard covenants' recommendation exceeds your individual DOA. You add a 'Policy/DOA check' section noting escalation is required.\n- **I:** The assessment omits accounts receivable aging — a required input under your credit policy for this sector.\n- **F:** You remove the unverified revenue growth sentence from the draft memo entirely.\n- **Y:** Because the corrected leverage ratio triggers a policy threshold, you escalate to your team lead before proceeding.\n\nThe final credit memo reflects only what you can support — and the AI saved you time on the initial read, not on the judgment.",
    "anti_pattern": "Copying an AI-generated company assessment directly into the credit memo because it looks professional and well-structured. The consequence: unverified figures (or figures the AI fabricated from partial inputs) enter the permanent credit file. Auditors find claims with no traceable source. A decision is made on a leverage ratio that was never cross-checked. The error is discovered post-approval, requiring a file correction, a potential credit committee review, and a note on the underwriter's quality record. The AI's polished formatting made the output feel finished — but formatting is not verification.",
    "takeaway": "If you cannot point to the source document that supports a claim, that claim does not belong in the credit file — the VERIFY Checklist is how you keep AI a productivity tool rather than a liability."
  },
  "uw_c4_tool_fluency": {
    "content_id": "rc_uw_c4_tool_fluency",
    "course_id": "uw_c4_tool_fluency",
    "concept_text": "Underwriters leave meetings with decisions, open items, and assigned actions — but those outcomes only become records if they are captured in the right tool, in the right order. The **Copilot Surface Selector** is a decision rule that tells you which Microsoft 365 surface to open first, based on where your input lives and what output you need next.\n\n**Step 1 — Teams Copilot** is your starting point when the meeting happened in Teams and a transcript or notes exist. Use it to generate a structured recap: decisions made, action items with named owners, and deadlines. This is the authoritative source of truth for the meeting.\n\n**Step 2 — Outlook Copilot** is your next surface when the recap needs to become a follow-up communication. Paste or reference the validated recap and prompt Outlook Copilot to draft an email that assigns actions, confirms deadlines, and flags anything still unresolved.\n\n**Step 3 — Word Copilot** applies when a formal memo artifact is required — for example, a committee note or a credit submission section that references the meeting outcome.\n\n**Step 4 — SharePoint** closes the loop. File the final, validated recap in the deal's designated folder using a consistent filename convention (e.g., `YYYY-MM-DD_DealName_MeetingRecap.docx`). This is the record governance step that makes the file retrievable and auditable.\n\nThe selector works because each surface is optimised for a specific input-output pair. Skipping a step or starting in the wrong surface creates gaps: missing decisions, unassigned actions, or records that never get filed. Chain the surfaces in order, validate at each handoff, and the meeting becomes a complete, traceable record.",
    "good_example": "**Before:** A deal forum ends. The underwriter opens Word and types notes from memory, missing two decisions that were made verbally.\n\n**After (Copilot Surface Selector applied):** The meeting ran in Teams → the underwriter opens **Teams Copilot** and prompts: *'Summarise this meeting as bullets under three headings: Decisions, Action Items (with owner and deadline), and Open Questions. Label anything uncertain as Needs confirmation.'* → The recap is reviewed and edited for accuracy → **Outlook Copilot** drafts a follow-up email embedding the action list and asking attendees to resolve 'Needs confirmation' items by Friday → The validated recap is saved to **SharePoint** under `/Deals/ActiveFiles/[DealName]/MeetingRecords/` as `2025-07-14_BluewaterLogistics_PipelineReview.docx`.",
    "anti_pattern": "Opening **Word** first and typing 'write meeting notes for today's pipeline review' when the transcript already exists in Teams. The consequence is a reconstructed document built from memory rather than the actual record — decisions get omitted, action owners are misattributed, and the file stored in SharePoint cannot be traced back to a verified source, creating an audit gap and potential compliance risk.",
    "takeaway": "Always start in the surface where the meeting input already lives — Teams first, then chain forward to Outlook and SharePoint — so every decision becomes a traceable, filed record without duplicated effort."
  },
  "uw_c5_capstone": {
    "content_id": "rc_uw_c5_capstone",
    "course_id": "uw_c5_capstone",
    "concept_text": "The capstone skill for an AI-enabled underwriter is not simply 'using AI' — it is **orchestrating AI safely across a full underwriting sprint**. The End-to-End AI Workflow gives you a four-stage structure that keeps you fast, accurate, and accountable.\n\n**Stage 1 — Intake & Triage:** Summarize a long third-party credit submission into structured risk flags, missing information, and open questions. Use a constrained Copilot prompt with a hard bullet limit so output stays scannable, not sprawling.\n\n**Stage 2 — Knowledge & Policy Pull:** Retrieve internal guidance — policy memos, underwriting templates, prior case summaries — using approved M365 surfaces (SharePoint, Teams, Copilot Business Chat). Never leave the approved environment to search for policy.\n\n**Stage 3 — Drafting & Options:** Draft your internal recommendation and client-facing update using tightly constrained prompts. Specify tone, length, audience, and what the AI must NOT invent. Treat every draft as a starting point, not a finished product.\n\n**Stage 4 — Verify & Finalize:** Before anything enters the credit file or reaches a client, apply VERIFY: check every factual claim against source documents, confirm policy alignment, validate that no confidential identifiers were exposed, and confirm the output sits within your delegated authority.\n\nThis workflow mirrors the intent of UW Streamlining tools: faster turnaround without transferring accountability to the model. The underwriter remains the decision-maker. AI accelerates the sprint; judgment closes it.\n\n**The winning pattern is: prompt → pull guidance → draft → verify.** Speed comes from the workflow, not from skipping steps.",
    "good_example": "**Before (unstructured):** An underwriter pastes a 40-page bank submission into Copilot Business Chat and asks, 'What do I need to know?' Output is a narrative summary with no structure, mixed priorities, and no flagged gaps.\n\n**After (End-to-End Workflow):**\n- *Stage 1 (Intake):* Prompt in Copilot Business Chat — 'You are a senior underwriter. Using only the text below, extract: (a) top 5 credit risk flags, (b) missing information required for a decision, (c) questions to resolve before approval. Maximum 12 bullets total. Do not infer beyond the text.' → Structured, scannable output in under 2 minutes.\n- *Stage 2 (Knowledge Pull):* Open SharePoint via Copilot, search 'infrastructure project finance policy 2024' and 'prior cases: toll road exposure' to retrieve the relevant policy memo and a comparable case template.\n- *Stage 3 (Draft):* Prompt Copilot to draft a 150-word internal recommendation using the risk flags and retrieved policy, specifying 'flag any claim you cannot source from the documents provided.'\n- *Stage 4 (Verify):* Cross-check every flagged risk against the original submission page references, confirm the recommendation aligns with delegated authority limits, and remove any residual client identifiers before filing.",
    "anti_pattern": "**Wrong behaviour — One-Shot Everything:** The underwriter writes a single prompt: 'Here is the full submission for Northgate Holdings. Summarize the risks, check against policy, draft a recommendation, suggest a pricing range, and write the client update.' \n\n**Consequences:** (1) The model hallucinates policy details it was never given. (2) The client name and exact exposure figures are sent to an unvetted surface. (3) The 'recommendation' contains invented precedent. (4) The pricing suggestion has no benchmark source. The underwriter, pressed for time, pastes the output directly into the credit file — creating a compliance and accuracy risk that is now on record. Skipping stages does not save time; it creates rework, audit exposure, and potential data incidents.",
    "takeaway": "Run every underwriting sprint through all four stages — prompt, pull guidance, draft, verify — because the workflow is what makes AI speed safe, not a shortcut around it."
  }
}