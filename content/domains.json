{
  "rm_prompting": {
    "domain_id": "prompting",
    "role_id": "rm",
    "title": "Prompting for Outcomes",
    "description": "Structuring AI prompts with context, constraints, format, and audience to produce outputs that are directly usable in RM workflows — briefing documents, emails, CRM notes, and talking points.",
    "level_0_label": "Unaware",
    "level_0_descriptor": "Has not used AI prompting in work tasks. Cannot describe what makes a prompt effective.",
    "level_1_label": "Explorer",
    "level_1_descriptor": "Writes basic prompts ('summarize this'). Output often requires heavy editing or is too generic to use.",
    "level_2_label": "Practitioner",
    "level_2_descriptor": "Uses structured prompts with context and format instructions. Output is usually usable with minor edits.",
    "level_3_label": "Proficient",
    "level_3_descriptor": "Adapts prompts for complex scenarios. Adds constraints proactively. Iterates when output misses the mark.",
    "level_4_label": "Champion",
    "level_4_descriptor": "Designs reusable prompt templates for team workflows. Coaches colleagues on prompting structure. Contributes new use cases."
  },
  "rm_verification": {
    "domain_id": "verification",
    "role_id": "rm",
    "title": "Verification and Judgment",
    "description": "Reviewing AI outputs critically before acting on them — catching hallucinations, incorrect dates, invented facts, and misattributed statements in meeting recaps, summaries, and CRM entries.",
    "level_0_label": "Unaware",
    "level_0_descriptor": "Treats AI outputs as accurate by default. Does not cross-reference against source material.",
    "level_1_label": "Explorer",
    "level_1_descriptor": "Reads AI output before using it, but does not systematically verify against independent sources.",
    "level_2_label": "Practitioner",
    "level_2_descriptor": "Routinely cross-references AI output against own notes. Removes or corrects unverifiable statements before logging.",
    "level_3_label": "Proficient",
    "level_3_descriptor": "Identifies subtle errors (plausible but wrong details). Adjusts prompts to reduce hallucination risk. Reviews with a skeptical lens.",
    "level_4_label": "Champion",
    "level_4_descriptor": "Develops verification checklists for team use. Can explain failure modes of AI summarization. Trains peers on review discipline."
  },
  "rm_data_safety": {
    "domain_id": "data_safety",
    "role_id": "rm",
    "title": "Data Safety and Compliance",
    "description": "Applying the public/non-public test before inputting client data into AI tools. Abstracting and anonymizing non-public information (credit figures, deal terms, private expansion plans) while still getting useful AI assistance.",
    "level_0_label": "Unaware",
    "level_0_descriptor": "Unaware of the non-public data rule or does not apply it in practice. May paste CRM records directly into public AI tools.",
    "level_1_label": "Explorer",
    "level_1_descriptor": "Knows the rule ('don't share non-public info') but cannot reliably distinguish public from non-public in real client scenarios.",
    "level_2_label": "Practitioner",
    "level_2_descriptor": "Applies the public/non-public test consistently. Abstracts client names and specific figures before prompting. Avoids policy violations.",
    "level_3_label": "Proficient",
    "level_3_descriptor": "Handles borderline cases confidently (e.g., NPS scores, internal notes, inferred financials). Rewrites prompts to preserve utility while removing risk.",
    "level_4_label": "Champion",
    "level_4_descriptor": "Identifies novel compliance risks in new use cases. Advises team on safe patterns. Acts as a data-safe AI usage model for peers."
  },
  "rm_tool_fluency": {
    "domain_id": "tool_fluency",
    "role_id": "rm",
    "title": "Tool Fluency (M365 + Copilot)",
    "description": "Choosing the right M365 Copilot surface (Outlook, Teams, Excel, Word/SharePoint) for each task and building multi-step workflows where output from one tool feeds the next — from meeting recap to CRM log to follow-up email.",
    "level_0_label": "Unaware",
    "level_0_descriptor": "Has not used Copilot features in M365 tools for work tasks. Unaware of which tools have AI capabilities.",
    "level_1_label": "Explorer",
    "level_1_descriptor": "Has tried one or two Copilot features (e.g., Outlook email draft). Does not connect tools into workflows.",
    "level_2_label": "Practitioner",
    "level_2_descriptor": "Uses at least three M365 Copilot surfaces regularly. Builds simple two-step workflows (e.g., Teams recap → C3 log).",
    "level_3_label": "Proficient",
    "level_3_descriptor": "Designs multi-step workflows across 3+ Copilot surfaces. Chooses the right entry point based on input type. Recovers gracefully when one step produces poor output.",
    "level_4_label": "Champion",
    "level_4_descriptor": "Documents and shares workflows with the team. Identifies new Copilot surfaces or features applicable to RM work. Trains peers on multi-step patterns."
  },
  "uw_prompting": {
    "domain_id": "prompting",
    "role_id": "uw",
    "title": "Prompting for Outcomes",
    "description": "Structuring AI prompts so outputs are directly usable in underwriting workflows — decision emails to policyholders/brokers, first-draft credit memos/submissions, covenant and conditions summaries, and concise internal recommendations for approvals — while staying aligned to underwriting standards and approved wording libraries.",
    "level_0_label": "Unaware",
    "level_0_descriptor": "Has not used AI for underwriting tasks. Cannot explain how to give context (deal type, audience, decision constraints) to get a useful draft.",
    "level_1_label": "Explorer",
    "level_1_descriptor": "Uses basic prompts (e.g., 'draft an email') with little context. Output is generic, may include inappropriate detail, and requires a full rewrite before it can be sent or filed.",
    "level_2_label": "Practitioner",
    "level_2_descriptor": "Writes structured prompts with: (1) underwriting context (product, decision type, what's known/unknown), (2) audience (client vs internal), (3) constraints (no internal model details; plain language), and (4) format (bullet sections, short rationale + next steps). Produces drafts that need only minor edits for accuracy and tone.",
    "level_3_label": "Proficient",
    "level_3_descriptor": "Adapts prompts for complex cases (partial approvals, exceptions, sensitive wording). Anticipates failure modes (over-disclosure, invented facts) and adds guardrails (cite only provided facts; ask clarifying questions). Iterates quickly to reach a compliant, client-ready message.",
    "level_4_label": "Champion",
    "level_4_descriptor": "Builds reusable prompt templates for common underwriting moments (decline explanation, info request, conditional approval). Coaches peers and contributes improvements to shared drafting standards (e.g., 'approved phrasing' playbooks)."
  },
  "uw_verification": {
    "domain_id": "verification",
    "role_id": "uw",
    "title": "Trust but Verify",
    "description": "Critically reviewing AI-assisted underwriting outputs before acting — validating financial figures, risk flags, policy alignment, and delegated authority. Ensures drafts match source documents and underwriting guidance and do not substitute for required human judgment.",
    "level_0_label": "Unaware",
    "level_0_descriptor": "Treats AI output as accurate by default. Might copy AI-generated risk rationale into the file without checking against the financials or credit report.",
    "level_1_label": "Explorer",
    "level_1_descriptor": "Skims AI output for obvious errors but does not systematically cross-check numbers, dates, or policy requirements.",
    "level_2_label": "Practitioner",
    "level_2_descriptor": "Uses a repeatable verification routine: cross-checks AI summaries against source docs (financial statements, credit reports, emails), confirms policy/DOA alignment, removes unverifiable statements, and documents what was checked before finalizing a decision.",
    "level_3_label": "Proficient",
    "level_3_descriptor": "Catches subtle errors (plausible-but-wrong covenants, swapped entities, invented thresholds). Tightens prompts to reduce hallucinations (e.g., 'use only the pasted text'). Escalates uncertainty rather than guessing.",
    "level_4_label": "Champion",
    "level_4_descriptor": "Creates verification checklists for the team (what must be true before approving). Trains peers on common AI failure modes in credit analysis and on how to document verification in the credit file."
  },
  "uw_data_safety": {
    "domain_id": "data_safety",
    "role_id": "uw",
    "title": "Safe Data Handling",
    "description": "Applying EDC's rules before using AI with underwriting data — protecting non-public client financials, deal terms, and any personal information. Uses only approved tools and follows policy requirements.",
    "level_0_label": "Unaware",
    "level_0_descriptor": "May paste client financial statements, names, or deal terms into public AI tools without considering classification.",
    "level_1_label": "Explorer",
    "level_1_descriptor": "Knows 'don't share confidential data' but struggles with gray areas (market rumors, internal ratings, partial identifiers). Prompts may still include too much detail.",
    "level_2_label": "Practitioner",
    "level_2_descriptor": "Consistently applies a 'public vs non-public' test, strips identifiers, abstracts numbers into ranges, and chooses the correct approved Copilot surface. Can explain why a prompt is safe.",
    "level_3_label": "Proficient",
    "level_3_descriptor": "Handles borderline cases confidently (e.g., combining multiple data points that could re-identify a client). Designs prompts that preserve usefulness while eliminating sensitive content, and adds audit-friendly notes on how AI was used.",
    "level_4_label": "Champion",
    "level_4_descriptor": "Spots new risk patterns (e.g., re-identification risk, hidden PII in attachments). Advises the team on safe prompt patterns and contributes examples to training and controls."
  },
  "uw_tool_fluency": {
    "domain_id": "tool_fluency",
    "role_id": "uw",
    "title": "Tool Fluency",
    "description": "Choosing the right Microsoft 365 Copilot surface for underwriting tasks and chaining outputs across tools — Teams meeting recap → Outlook follow-up email → SharePoint filing → Word memo edits → Excel checks — while keeping the credit file complete and compliant.",
    "level_0_label": "Unaware",
    "level_0_descriptor": "Has not used Copilot features in Outlook/Teams/Word/Excel. Doesn't know which tool is best for which task.",
    "level_1_label": "Explorer",
    "level_1_descriptor": "Uses one Copilot feature occasionally (e.g., draft an email) but does not connect steps (meeting recap doesn't flow into record updates).",
    "level_2_label": "Practitioner",
    "level_2_descriptor": "Uses at least three Copilot surfaces and completes simple chains (Teams recap → Outlook email → save note to SharePoint). Chooses a surface based on input type (transcript vs document vs spreadsheet).",
    "level_3_label": "Proficient",
    "level_3_descriptor": "Designs multi-step workflows across 3+ tools, recovers when one step fails (e.g., recap misses decisions), and knows when to switch surfaces (Excel for numbers, Word for narrative).",
    "level_4_label": "Champion",
    "level_4_descriptor": "Documents best-practice tool chains for common underwriting cycles (triage day, credit committee week, end-of-week housekeeping). Coaches peers and shares new Copilot features that reduce repetitive work."
  }
}